# Story 6.2: Implement Engine Execution and Result Aggregation

Status: done

## Story

As an **attorney**,
I want **complex queries to use multiple engines with combined results**,
So that **I get comprehensive answers from the right combination of engines**.

## Acceptance Criteria

1. **Given** a query requires multiple engines
   **When** orchestration runs
   **Then** engines are executed in the correct order (some parallel, some sequential)
   **And** results are aggregated into a unified response

2. **Given** engines can run in parallel (e.g., Citation + Timeline)
   **When** orchestration runs
   **Then** both engines execute simultaneously
   **And** response time is optimized

3. **Given** engines have dependencies (e.g., Contradiction needs MIG data)
   **When** orchestration runs
   **Then** dependent engines wait for prerequisites
   **And** correct execution order is maintained

4. **Given** all engine results are ready
   **When** aggregation runs
   **Then** results are combined into a coherent response
   **And** sources from all engines are included

## Tasks / Subtasks

- [x] Task 1: Create engine execution models (AC: #1-4)
  - [x] 1.1: Add `EngineExecutionRequest` model with `matter_id`, `query`, `engines`, `context`
  - [x] 1.2: Add `EngineExecutionResult` model per engine with `engine`, `success`, `data`, `error`, `execution_time_ms`
  - [x] 1.3: Add `OrchestratorResult` model for aggregated response with all engine results
  - [x] 1.4: Add `ExecutionPlan` model with `parallel_groups`, `execution_order`
  - [x] 1.5: Add models to `backend/app/models/orchestrator.py` (extend existing)

- [x] Task 2: Create engine executor with parallel execution (AC: #1-2)
  - [x] 2.1: Create `EngineExecutor` class in `backend/app/engines/orchestrator/executor.py` (NEW)
  - [x] 2.2: Implement `execute_engines()` - orchestrate multiple engine calls
  - [x] 2.3: Implement `_execute_parallel_group()` - run independent engines concurrently with `asyncio.gather`
  - [x] 2.4: Implement `_execute_single_engine()` - call individual engine with error handling
  - [x] 2.5: Add `get_engine_executor()` factory function
  - [x] 2.6: Add execution timing and cost tracking

- [x] Task 3: Implement execution planning and dependency handling (AC: #3)
  - [x] 3.1: Create `ExecutionPlanner` class in `backend/app/engines/orchestrator/planner.py` (NEW)
  - [x] 3.2: Define `ENGINE_DEPENDENCIES` mapping (which engines depend on others)
  - [x] 3.3: Implement `create_execution_plan()` - determine parallel vs sequential groups
  - [x] 3.4: Implement `_resolve_dependencies()` - topological sort for execution order
  - [x] 3.5: Implement `_can_run_parallel()` - check if engines have no shared dependencies
  - [x] 3.6: Add `get_execution_planner()` factory function

- [x] Task 4: Implement result aggregation (AC: #4)
  - [x] 4.1: Create `ResultAggregator` class in `backend/app/engines/orchestrator/aggregator.py` (NEW)
  - [x] 4.2: Implement `aggregate_results()` - combine results from multiple engines
  - [x] 4.3: Implement `_merge_sources()` - deduplicate and combine source references
  - [x] 4.4: Implement `_calculate_overall_confidence()` - weighted average of engine confidences
  - [x] 4.5: Implement `_format_unified_response()` - create coherent combined response
  - [x] 4.6: Add `get_result_aggregator()` factory function

- [x] Task 5: Create engine adapters for existing engines (AC: #1-4)
  - [x] 5.1: Create `EngineAdapter` abstract base in `backend/app/engines/orchestrator/adapters.py` (NEW)
  - [x] 5.2: Implement `CitationEngineAdapter` - wrap citation engine for orchestrator
  - [x] 5.3: Implement `TimelineEngineAdapter` - wrap timeline engine for orchestrator
  - [x] 5.4: Implement `ContradictionEngineAdapter` - wrap contradiction engine for orchestrator
  - [x] 5.5: Implement `RAGEngineAdapter` - wrap RAG search for orchestrator
  - [x] 5.6: Add adapter registry for engine type to adapter mapping

- [x] Task 6: Integrate with Intent Analyzer (Story 6-1) (AC: #1)
  - [x] 6.1: Create `QueryOrchestrator` class in `backend/app/engines/orchestrator/orchestrator.py` (NEW)
  - [x] 6.2: Implement `process_query()` - full pipeline: intent â†’ plan â†’ execute â†’ aggregate
  - [x] 6.3: Wire up IntentAnalyzer â†’ ExecutionPlanner â†’ EngineExecutor â†’ ResultAggregator
  - [x] 6.4: Add `get_query_orchestrator()` factory function
  - [x] 6.5: Export from `engines/orchestrator/__init__.py`

- [x] Task 7: Write comprehensive tests (AC: #1-4)
  - [x] 7.1: Unit tests for `ExecutionPlanner` with dependency resolution
  - [x] 7.2: Unit tests for `EngineExecutor` with mocked engines
  - [x] 7.3: Test parallel execution optimization (Citation + Timeline run together)
  - [x] 7.4: Test sequential execution for dependencies (Contradiction after prerequisites)
  - [x] 7.5: Unit tests for `ResultAggregator` with source merging
  - [x] 7.6: Integration test for full orchestration pipeline
  - [x] 7.7: Test error handling (one engine fails, others succeed)
  - [x] 7.8: Test matter isolation security (CRITICAL)
  - [x] 7.9: Test timeout handling for slow engines

## Dev Notes

### Architecture Compliance

This story implements the **second stage** of the **Engine Orchestrator** (Epic 6):

```
INTENT ANALYSIS (6-1) âœ… â†’ ENGINE EXECUTION (6-2) ðŸ‘ˆ â†’ AUDIT LOGGING (6-3)
```

The orchestrator follows the established engine/service/route pattern from previous epics.

### Critical Implementation Details

1. **Engine Execution Pipeline**

   ```
   User Query
       â†“
   IntentAnalyzer (Story 6-1)
       â†“ IntentClassification {intent, confidence, required_engines}
   ExecutionPlanner
       â†“ ExecutionPlan {parallel_groups, execution_order}
   EngineExecutor
       â†“ [EngineResult, EngineResult, ...]
   ResultAggregator
       â†“ OrchestratorResult {unified_response, sources, confidence}
   ```

2. **Engine Dependencies (CRITICAL - Task 3.2)**

   Define which engines require data from others:

   ```python
   ENGINE_DEPENDENCIES: dict[EngineType, list[EngineType]] = {
       EngineType.CITATION: [],  # Independent - can always run
       EngineType.TIMELINE: [],  # Independent - can always run
       EngineType.CONTRADICTION: [],  # Independent - uses MIG data directly from DB
       EngineType.RAG: [],  # Independent - always available
   }
   ```

   **Note:** All current engines are independent. They query the database directly
   for MIG data rather than depending on other engine outputs. This allows
   maximum parallelization.

   **Future consideration:** If we add engines that depend on another engine's
   OUTPUT (not just shared data), we'd need to add them here.

3. **Parallel Execution Strategy (AC: #2)**

   Since all engines are currently independent, they can ALL run in parallel:

   ```python
   async def execute_engines(
       self,
       matter_id: str,
       query: str,
       engines: list[EngineType],
   ) -> list[EngineExecutionResult]:
       """Execute all engines in parallel."""
       tasks = [
           self._execute_single_engine(matter_id, query, engine)
           for engine in engines
       ]
       results = await asyncio.gather(*tasks, return_exceptions=True)
       return self._process_results(results, engines)
   ```

4. **Engine Adapter Pattern (Task 5)**

   Create adapters to normalize engine interfaces:

   ```python
   class EngineAdapter(ABC):
       """Abstract adapter for normalizing engine interfaces."""

       @abstractmethod
       async def execute(
           self,
           matter_id: str,
           query: str,
           context: dict | None = None,
       ) -> EngineExecutionResult:
           """Execute the underlying engine and return normalized result."""
           ...

   class CitationEngineAdapter(EngineAdapter):
       """Adapter for Citation Verification Engine (Epic 3)."""

       def __init__(self):
           # Import here to avoid circular imports
           from app.engines.citation import get_citation_verifier

           self._engine = None

       async def execute(
           self,
           matter_id: str,
           query: str,
           context: dict | None = None,
       ) -> EngineExecutionResult:
           start_time = time.time()
           try:
               # Call citation engine with appropriate method
               # Citation engine has verify_citation(), extract_citations()
               # Choose based on query type
               result = await self._get_engine().verify_citations_for_query(
                   matter_id=matter_id,
                   query=query,
               )
               return EngineExecutionResult(
                   engine=EngineType.CITATION,
                   success=True,
                   data=result.model_dump(),
                   execution_time_ms=int((time.time() - start_time) * 1000),
               )
           except Exception as e:
               return EngineExecutionResult(
                   engine=EngineType.CITATION,
                   success=False,
                   error=str(e),
                   execution_time_ms=int((time.time() - start_time) * 1000),
               )
   ```

5. **Result Aggregation Strategy (AC: #4)**

   Combine results from multiple engines:

   ```python
   class ResultAggregator:
       """Aggregates results from multiple engines into unified response."""

       def aggregate_results(
           self,
           results: list[EngineExecutionResult],
           query: str,
       ) -> OrchestratorResult:
           """Combine engine results into coherent response."""
           # Collect successful results
           successful = [r for r in results if r.success]
           failed = [r for r in results if not r.success]

           # Merge sources from all engines
           all_sources = self._merge_sources(successful)

           # Calculate weighted confidence
           overall_confidence = self._calculate_overall_confidence(successful)

           # Format unified response
           unified_response = self._format_unified_response(successful, query)

           return OrchestratorResult(
               query=query,
               successful_engines=[r.engine for r in successful],
               failed_engines=[r.engine for r in failed],
               unified_response=unified_response,
               sources=all_sources,
               confidence=overall_confidence,
               engine_results=results,
               total_execution_time_ms=sum(r.execution_time_ms for r in results),
           )
   ```

6. **Error Handling Strategy**

   Graceful degradation when engines fail:

   ```python
   # If one engine fails, still return results from others
   async def _execute_single_engine(
       self,
       matter_id: str,
       query: str,
       engine: EngineType,
   ) -> EngineExecutionResult:
       """Execute single engine with error handling."""
       adapter = self._get_adapter(engine)
       try:
           return await asyncio.wait_for(
               adapter.execute(matter_id, query),
               timeout=30.0,  # 30 second timeout per engine
           )
       except asyncio.TimeoutError:
           logger.warning(
               "engine_timeout",
               engine=engine.value,
               matter_id=matter_id,
           )
           return EngineExecutionResult(
               engine=engine,
               success=False,
               error="Engine timed out after 30 seconds",
               execution_time_ms=30000,
           )
       except Exception as e:
           logger.error(
               "engine_execution_failed",
               engine=engine.value,
               error=str(e),
           )
           return EngineExecutionResult(
               engine=engine,
               success=False,
               error=str(e),
               execution_time_ms=0,
           )
   ```

7. **Existing Code to Reuse (CRITICAL - DO NOT REINVENT)**

   | Component | Location | Purpose |
   |-----------|----------|---------|
   | `IntentAnalyzer` | `app/engines/orchestrator/intent_analyzer.py` | Query classification (Story 6-1) |
   | `EngineType` enum | `app/models/orchestrator.py` | Engine identifiers |
   | `IntentClassification` | `app/models/orchestrator.py` | Classification result |
   | `EngineBase` | `app/engines/base.py` | Base engine pattern |
   | `EngineInput/Output` | `app/engines/base.py` | I/O models |
   | `get_*` factory pattern | All engines | Dependency injection |
   | structlog | All modules | Structured logging |

### File Structure

Extend the orchestrator engine structure:

```
backend/app/
â”œâ”€â”€ engines/
â”‚   â”œâ”€â”€ orchestrator/                    # Epic 6
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # Exports (update)
â”‚   â”‚   â”œâ”€â”€ intent_analyzer.py           # Story 6-1 âœ…
â”‚   â”‚   â”œâ”€â”€ prompts.py                   # Intent classification prompts âœ…
â”‚   â”‚   â”œâ”€â”€ planner.py                   # Story 6-2 (NEW) - Execution planning
â”‚   â”‚   â”œâ”€â”€ executor.py                  # Story 6-2 (NEW) - Engine execution
â”‚   â”‚   â”œâ”€â”€ aggregator.py                # Story 6-2 (NEW) - Result aggregation
â”‚   â”‚   â”œâ”€â”€ adapters.py                  # Story 6-2 (NEW) - Engine adapters
â”‚   â”‚   â””â”€â”€ orchestrator.py              # Story 6-2 (NEW) - Main orchestrator
â”‚   â”œâ”€â”€ citation/                        # Epic 3 âœ…
â”‚   â”œâ”€â”€ timeline/                        # Epic 4 âœ…
â”‚   â””â”€â”€ contradiction/                   # Epic 5 âœ…
â”œâ”€â”€ models/
â”‚   â””â”€â”€ orchestrator.py                  # Extend with execution models
â””â”€â”€ tests/
    â””â”€â”€ engines/
        â””â”€â”€ orchestrator/
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ test_intent_analyzer.py  # Story 6-1 âœ… (41 tests)
            â”œâ”€â”€ test_planner.py          # Story 6-2 (NEW)
            â”œâ”€â”€ test_executor.py         # Story 6-2 (NEW)
            â”œâ”€â”€ test_aggregator.py       # Story 6-2 (NEW)
            â””â”€â”€ test_orchestrator.py     # Story 6-2 (NEW) - Integration
```

### Model Definitions

Add to `backend/app/models/orchestrator.py`:

```python
# =============================================================================
# Story 6-2: Engine Execution Models
# =============================================================================

class EngineExecutionRequest(BaseModel):
    """Request for engine execution via orchestrator."""

    matter_id: str = Field(min_length=1, description="Matter UUID")
    query: str = Field(min_length=1, description="User's query")
    engines: list[EngineType] = Field(description="Engines to execute")
    context: dict[str, Any] | None = Field(
        default=None,
        description="Optional context from conversation history",
    )


class EngineExecutionResult(BaseModel):
    """Result from a single engine execution."""

    engine: EngineType = Field(description="Engine that produced this result")
    success: bool = Field(description="Whether execution succeeded")
    data: dict[str, Any] | None = Field(
        default=None,
        description="Engine-specific result data",
    )
    error: str | None = Field(
        default=None,
        description="Error message if execution failed",
    )
    execution_time_ms: int = Field(
        default=0,
        description="Time taken to execute in milliseconds",
    )
    confidence: float | None = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Confidence score from engine (if applicable)",
    )


class ExecutionPlan(BaseModel):
    """Plan for executing engines."""

    parallel_groups: list[list[EngineType]] = Field(
        description="Groups of engines that can run in parallel. "
                    "Groups are executed sequentially, engines within groups in parallel.",
    )
    total_engines: int = Field(description="Total number of engines to execute")
    estimated_parallelism: float = Field(
        description="Parallelism factor (1.0 = all sequential, higher = more parallel)",
    )


class SourceReference(BaseModel):
    """Reference to a source document or chunk."""

    document_id: str = Field(description="Source document UUID")
    document_name: str | None = Field(default=None, description="Document filename")
    chunk_id: str | None = Field(default=None, description="Specific chunk UUID")
    page_number: int | None = Field(default=None, description="Page number if applicable")
    text_preview: str | None = Field(default=None, description="Preview of source text")
    confidence: float | None = Field(default=None, description="Source relevance score")
    engine: EngineType | None = Field(default=None, description="Engine that found this source")


class OrchestratorResult(BaseModel):
    """Aggregated result from orchestrator execution."""

    matter_id: str = Field(description="Matter UUID")
    query: str = Field(description="Original query")
    successful_engines: list[EngineType] = Field(
        description="Engines that executed successfully",
    )
    failed_engines: list[EngineType] = Field(
        default_factory=list,
        description="Engines that failed to execute",
    )
    unified_response: str = Field(
        description="Combined human-readable response from all engines",
    )
    sources: list[SourceReference] = Field(
        default_factory=list,
        description="Deduplicated source references from all engines",
    )
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Overall confidence (weighted average of engine confidences)",
    )
    engine_results: list[EngineExecutionResult] = Field(
        description="Individual results from each engine",
    )
    total_execution_time_ms: int = Field(
        description="Total time including all parallel executions",
    )
    wall_clock_time_ms: int = Field(
        default=0,
        description="Actual wall clock time (accounts for parallelism)",
    )


class OrchestratorResponse(BaseModel):
    """API response wrapper for orchestrator execution.

    Follows project-context.md API response format.
    """

    data: OrchestratorResult = Field(description="Orchestration result")
```

### Previous Story (6-1) Learnings

From Story 6-1 implementation:

1. **Factory pattern**: Use `get_*()` functions for dependency injection
2. **Structured logging**: Use structlog for all logging
3. **Cost tracking**: Track execution time and resources
4. **Error handling**: Graceful degradation with detailed error messages
5. **Clean models**: Use Pydantic v2 with type hints and Field descriptions
6. **Test coverage**: Include edge cases, security tests, integration tests
7. **Matter isolation**: ALWAYS verify matter_id in all operations

### Git Intelligence

Recent commit pattern:
- `feat(orchestrator): implement query intent analysis engine (Story 6-1)`
- Pattern: `feat(domain): description (Story X-Y)`
- Code review: `fix(review): address code review issues for Story X-Y`

### Testing Requirements

Per project-context.md:
- Backend: `tests/engines/orchestrator/` directory
- Use pytest-asyncio for async tests
- Mock engine calls (don't call real engines in tests)
- Include matter isolation test (CRITICAL)

**Test Files to Create:**
- `tests/engines/orchestrator/test_planner.py`
- `tests/engines/orchestrator/test_executor.py`
- `tests/engines/orchestrator/test_aggregator.py`
- `tests/engines/orchestrator/test_orchestrator.py`

**Minimum Test Cases:**

```python
# test_planner.py
@pytest.mark.asyncio
async def test_create_plan_all_parallel():
    """All independent engines should run in parallel."""
    planner = get_execution_planner()
    plan = planner.create_execution_plan([
        EngineType.CITATION,
        EngineType.TIMELINE,
        EngineType.RAG,
    ])
    # All engines in single parallel group
    assert len(plan.parallel_groups) == 1
    assert len(plan.parallel_groups[0]) == 3


@pytest.mark.asyncio
async def test_create_plan_single_engine():
    """Single engine should have one group."""
    planner = get_execution_planner()
    plan = planner.create_execution_plan([EngineType.CITATION])
    assert len(plan.parallel_groups) == 1
    assert plan.parallel_groups[0] == [EngineType.CITATION]


# test_executor.py
@pytest.mark.asyncio
async def test_execute_parallel_engines(mock_adapters):
    """Multiple engines should execute in parallel."""
    executor = get_engine_executor()
    results = await executor.execute_engines(
        matter_id="matter-123",
        query="What are the citations and timeline?",
        engines=[EngineType.CITATION, EngineType.TIMELINE],
    )
    assert len(results) == 2
    # Verify both executed (mocked)
    assert all(r.success for r in results)


@pytest.mark.asyncio
async def test_execute_with_one_failure(mock_adapters_with_failure):
    """If one engine fails, others should still succeed."""
    executor = get_engine_executor()
    results = await executor.execute_engines(
        matter_id="matter-123",
        query="Complex query",
        engines=[EngineType.CITATION, EngineType.TIMELINE],
    )
    # One success, one failure
    assert sum(1 for r in results if r.success) == 1
    assert sum(1 for r in results if not r.success) == 1


@pytest.mark.asyncio
async def test_execute_timeout_handling(mock_slow_adapter):
    """Slow engines should timeout gracefully."""
    executor = get_engine_executor()
    result = await executor._execute_single_engine(
        matter_id="matter-123",
        query="Query",
        engine=EngineType.CITATION,
    )
    assert not result.success
    assert "timeout" in result.error.lower()


# test_aggregator.py
@pytest.mark.asyncio
async def test_aggregate_multiple_results():
    """Results from multiple engines should be combined."""
    aggregator = get_result_aggregator()
    results = [
        EngineExecutionResult(
            engine=EngineType.CITATION,
            success=True,
            data={"citations": [{"act": "NI Act", "section": "138"}]},
            confidence=0.9,
            execution_time_ms=100,
        ),
        EngineExecutionResult(
            engine=EngineType.TIMELINE,
            success=True,
            data={"events": [{"date": "2024-01-01", "event": "Filing"}]},
            confidence=0.85,
            execution_time_ms=150,
        ),
    ]
    aggregated = aggregator.aggregate_results(results, "test query")
    assert EngineType.CITATION in aggregated.successful_engines
    assert EngineType.TIMELINE in aggregated.successful_engines
    assert aggregated.confidence > 0


@pytest.mark.asyncio
async def test_merge_sources_deduplication():
    """Duplicate sources should be deduplicated."""
    aggregator = get_result_aggregator()
    # Create results with overlapping sources
    # Verify deduplication logic


# test_orchestrator.py (integration)
@pytest.mark.asyncio
async def test_full_pipeline_citation_query(mock_all_components):
    """Full pipeline for citation-specific query."""
    orchestrator = get_query_orchestrator()
    result = await orchestrator.process_query(
        matter_id="matter-123",
        query="What citations are in this case?",
    )
    assert result.successful_engines
    assert result.unified_response


@pytest.mark.asyncio
async def test_full_pipeline_multi_engine(mock_all_components):
    """Full pipeline for ambiguous query requiring multiple engines."""
    orchestrator = get_query_orchestrator()
    result = await orchestrator.process_query(
        matter_id="matter-123",
        query="Tell me about the citations and timeline",
    )
    # Should use multiple engines
    assert len(result.successful_engines) > 1


@pytest.mark.asyncio
async def test_matter_isolation():
    """Verify matter_id is propagated to all engines."""
    # CRITICAL: Security test
    orchestrator = get_query_orchestrator()
    result = await orchestrator.process_query(
        matter_id="matter-123",
        query="Any data?",
    )
    assert result.matter_id == "matter-123"
    # Verify each engine result has correct matter_id
```

### Performance Considerations

1. **Parallelism**: Maximize concurrent engine execution with `asyncio.gather`
2. **Timeouts**: 30 second timeout per engine to prevent blocking
3. **Early return**: If all required engines fail, return error immediately
4. **Caching**: Future consideration - cache results for identical queries (Story 7-5)

### API Design Preview (Future - Story 6-3 or Epic 11)

The orchestrator will be called by the chat API:

```python
# Future: Q&A Panel integration
@router.post("/api/chat/query")
async def process_query(
    request: QueryRequest,
    current_user: User = Depends(get_current_user),
    orchestrator: QueryOrchestrator = Depends(get_query_orchestrator),
) -> QueryResponse:
    result = await orchestrator.process_query(
        matter_id=request.matter_id,
        query=request.query,
        context=request.context,
    )
    return QueryResponse(data=result)
```

### Project Structure Notes

- Orchestrator logic in `engines/orchestrator/`
- New models extend `models/orchestrator.py`
- Tests in `tests/engines/orchestrator/`
- Follow existing factory pattern (`get_*()` functions)
- All new classes use structlog for logging

### References

- [Project Context](../_bmad-output/project-context.md) - LLM routing rules, naming conventions
- [Architecture: Engine Orchestrator](../_bmad-output/architecture.md) - ADR-003 engine design
- [Epic 6 Definition](../_bmad-output/project-planning-artifacts/epics.md) - Story requirements
- [Story 6-1 Implementation](./6-1-query-intent-analysis.md) - IntentAnalyzer pattern
- [Engine Base Class](../backend/app/engines/base.py) - Base class for engines
- [Orchestrator Models](../backend/app/models/orchestrator.py) - Existing orchestrator models

## Dev Agent Record

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

N/A - Clean implementation with no major issues.

### Completion Notes List

- All 7 tasks completed successfully
- 61 new tests written for Story 6-2 (102 total orchestrator tests including Story 6-1)
- All engines currently independent - maximum parallelization achieved
- Engine adapters wrap Citation, Timeline, Contradiction, and RAG engines
- QueryOrchestrator provides full pipeline: intent â†’ plan â†’ execute â†’ aggregate
- Matter isolation verified through propagation tests (CRITICAL)
- 30-second timeout per engine for graceful degradation
- Weighted confidence calculation for multi-engine results

### File List

**New Files:**
- `backend/app/engines/orchestrator/planner.py` - ExecutionPlanner with dependency handling
- `backend/app/engines/orchestrator/executor.py` - EngineExecutor with parallel execution
- `backend/app/engines/orchestrator/aggregator.py` - ResultAggregator for combining results
- `backend/app/engines/orchestrator/adapters.py` - Engine adapters for Citation, Timeline, Contradiction, RAG
- `backend/app/engines/orchestrator/orchestrator.py` - QueryOrchestrator main pipeline
- `backend/tests/engines/orchestrator/test_planner.py` - 13 tests
- `backend/tests/engines/orchestrator/test_executor.py` - 12 tests
- `backend/tests/engines/orchestrator/test_aggregator.py` - 20 tests
- `backend/tests/engines/orchestrator/test_orchestrator.py` - 16 tests
- `backend/tests/engines/orchestrator/test_adapters.py` - 27 tests (added during code review)

**Modified Files:**
- `backend/app/models/orchestrator.py` - Added Story 6-2 models (EngineExecutionRequest, EngineExecutionResult, ExecutionPlan, SourceReference, OrchestratorResult, OrchestratorResponse)
- `backend/app/engines/orchestrator/__init__.py` - Exported Story 6-2 components

## Senior Developer Review (AI)

**Reviewer:** Claude Opus 4.5 (Adversarial Code Review)
**Date:** 2026-01-14
**Outcome:** âœ… APPROVED (after fixes)

### Issues Found & Fixed

| Severity | Issue | Resolution |
|----------|-------|------------|
| ðŸ”´ HIGH | `ContradictionEngineAdapter` called non-existent `get_entity_statements()` method | Fixed to call `get_statements_for_entity()` with correct parameter order |
| ðŸŸ¡ MEDIUM | Unused `SourceReference` import in adapters.py | Removed unused import |
| ðŸŸ¡ MEDIUM | No unit tests for adapter implementations | Added 27 tests in `test_adapters.py` |
| ðŸŸ¢ LOW | Magic numbers for RAG limits and timeline page size | Extracted to constants: `RAG_SEARCH_LIMIT`, `RAG_RERANK_TOP_N`, `TIMELINE_DEFAULT_PAGE_SIZE` |

### Test Coverage Summary

- **Total orchestrator tests:** 129 (was 102, added 27 adapter tests)
- **All tests passing:** âœ…
- **Matter isolation:** Verified in adapters, executor, and orchestrator tests

## Change Log

- 2026-01-14: Story 6-2 implementation complete - Engine Execution and Result Aggregation
- 2026-01-14: Code review fixes - Fixed ContradictionEngineAdapter method name, removed unused import, added adapter tests, extracted constants
