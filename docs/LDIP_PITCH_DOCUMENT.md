# LDIP: Legal Document Intelligence Platform
## Product Pitch Document

**Version:** 1.0  
**Date:** 2025-01-XX  
**Purpose:** Comprehensive overview for team members and stakeholders

---

## Table of Contents

1. [The Problem](#the-problem)
2. [The Solution](#the-solution)
3. [Real-World Example](#real-world-example)
4. [How It Works](#how-it-works)
5. [Key Features](#key-features)
6. [Technical Architecture](#technical-architecture)
7. [Value Proposition](#value-proposition)
8. [Next Steps](#next-steps)

---

## The Problem

### The Challenge Legal Teams Face

Legal teams working on complex matters face a critical challenge: **analyzing hundreds of documents across years of litigation to find patterns, inconsistencies, and missing information.**

#### Current Pain Points

**1. Manual Analysis is Extremely Time-Consuming**
- Reading 100+ document case files takes **50-85 hours**
- Cross-referencing against Act provisions is tedious
- Timeline construction requires manual effort
- Easy to miss connections across documents

**2. Junior Lawyers Have Limitations**
- Limited experience spotting subtle violations
- May not know all applicable Act provisions
- Can miss patterns across multiple cases
- Risk of overlooking hidden caveats

**3. Critical Issues Get Missed**
- Hidden agendas hard to detect
- Multi-party coordination failures overlooked
- Statistical anomalies not obvious
- Novel violations not recognized
- Contradictions across documents go unnoticed

**4. No Systematic Approach**
- Finding similar precedents takes days
- Cross-case analysis is manual
- Pattern discovery relies on memory
- No systematic way to verify citations
- No automated consistency checking

### The Cost of Missing Critical Information

**Real Impact:**
- Cases lost due to missed contradictions
- Malpractice risks from incomplete analysis
- Client trust damaged by oversight
- Hours wasted on manual document review
- Junior lawyers overwhelmed by volume

---

## The Solution

### LDIP: Your Forensic Reading Assistant

**LDIP (Legal Document Intelligence Platform)** is an AI-assisted, attorney-supervised analysis system that:

‚úÖ **Reads** hundreds of legal documents automatically  
‚úÖ **Extracts** factual insights and patterns  
‚úÖ **Detects** inconsistencies, missing documents, timeline anomalies  
‚úÖ **Verifies** citations against Acts  
‚úÖ **Surfaces** signals requiring attorney investigation  

**What LDIP Does NOT Do:**
- ‚ùå Provide legal advice
- ‚ùå Make legal conclusions
- ‚ùå Does NOT determine ownership, entitlement, compliance, or legality.
- ‚ùå Predict case outcomes
- ‚ùå Assign fault or blame
- ‚ùå Suggest legal strategy
- ‚ùå Make moral judgments
- ‚ùå Use language implying legal conclusions ("violates", "illegal", "liable", "guilty")

**What LDIP DOES:**
- ‚úîÔ∏è Extract facts with citations (document + page + line)
- ‚úîÔ∏è Highlight inconsistencies
- ‚úîÔ∏è Flag missing information
- ‚úîÔ∏è Map events and timelines
- ‚úîÔ∏è Surface patterns for attorney review
- ‚úîÔ∏è Detect admissions and non-denials
- ‚úîÔ∏è Identify pleading-document mismatches
- ‚úîÔ∏è Provide case orientation (court, stage, last order, next date)
- ‚úîÔ∏è Extract operative directions from latest orders
- ‚úîÔ∏è Generate junior case notes (facts-only)
- ‚úîÔ∏è Maintain risk & weakness registers

### Core Value Proposition

**Think of LDIP as:** A "forensic reading assistant" that reads hundreds of documents and surfaces signals that a human lawyer may want to investigate further.

**Key Differentiators:**

1. **Evidence-First Architecture** - Every claim tied to document, page, and line number
2. **Matter Isolation** - Strict ethical walls, no cross-matter leakage
3. **Eight Specialized Engines** - Citation, Timeline, Consistency, Documentation, Process Chain, Entity Authenticity, Admissions & Non-Denial, Pleading Mismatch
4. **Neutral Fact Extraction** - No legal conclusions, only factual patterns
5. **Attorney Supervision Built-In** - All findings require human verification
6. **Query Guardrails** - Prevents misuse, rewrites unsafe queries
7. **Language Policing** - Real-time enforcement of neutral language
8. **Case Orientation** - Day-zero clarity on court, stage, last order, next date
9. **Junior Lawyer Support** - Case notes, risk registers, workflow tools
10. **Stress Test Compliant** - Survives adversarial scrutiny from all angles

---

## Real-World Example

### The Nirav Jobalia Share Sale Case

This real case demonstrates exactly why LDIP is needed.

#### Background

Nirav Jobalia converted physical shares to dematerialized (demat) form and sold them. The shares were referenced as benami in some documents, indicating recorded ownership inconsistencies across documents. **Multiple sophisticated parties failed to catch this pattern during an 8-10 month process.**

#### The Process Deviation Chain

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PROCESS DEVIATION CHAIN: 8-10 Months of Documented Steps ‚îÇ
‚îÇ  vs Expected Template                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Step 1: Physical to Demat Conversion Request
   ‚îú‚îÄ Nirav claims physical shares for dematerialization
   ‚îú‚îÄ ‚ùå No document evidencing ownership verification was found in the uploaded materials.
   ‚îî‚îÄ ‚úÖ Was NOT challenged

Step 2: Missing Expected Documentation
   ‚îú‚îÄ ‚ùå No document evidencing verification of payment proof
   ‚îú‚îÄ ‚ùå No document evidencing chain of title documentation
   ‚îî‚îÄ ‚úÖ Request proceeded without verification

Step 3: Missing Expected Step
   ‚îú‚îÄ ‚ùå No documentary evidence found for expected institutional step.
   ‚îî‚îÄ ‚ùå Standard checks were not performed

Step 4: Benami Reference Pattern
   ‚îú‚îÄ ‚ö†Ô∏è References to benami classification appear in some documents but are not referenced in later procedural records.
   ‚îú‚îÄ ‚ö†Ô∏è Mehta family declared these as benami in other documents
   ‚îú‚îÄ ‚ö†Ô∏è Company records showed disputed ownership
   ‚îî‚îÄ ‚úÖ All parties proceeded anyway

Step 5: Demat Account Transfer
   ‚îú‚îÄ ‚ö†Ô∏è Shares entered Nirav's demat account
   ‚îî‚îÄ ‚ùì No document describing the approval basis was found.

Step 6: Share Sale Completed
   ‚îú‚îÄ ‚úÖ Shares sold to third party
   ‚îî‚îÄ ‚úÖ Transaction completed before notification

Step 7: Missing Notification Documentation
   ‚îú‚îÄ ‚ùå No document evidencing notification sent to interested parties (Mehtas)
   ‚îú‚îÄ ‚ùå Required by Torts Act Section 15
   ‚îú‚îÄ ‚ö†Ô∏è Mehtas discovered sale 3 months after completion
   ‚îî‚îÄ ‚ùå Transaction now irreversible
```

#### What Junior Lawyers Needed to Find

**LDIP would have detected:**

1. ‚úÖ **Benami List Reference** - Shares mentioned as benami across multiple documents
2. ‚úÖ **Missing Notification** - No documentation of required notification
3. ‚úÖ **Timeline Anomaly** - 9 months vs typical 2-3 months (red flag)
4. ‚úÖ **Multi-Party Pattern** - Custodian + Company + Registrar all show missing expected actions simultaneously
5. ‚úÖ **Comparison Pattern** - Kalpana Jobalia case had different outcome (why?)
6. ‚úÖ **Missing Payment Proof** - No evidence of original purchase
7. ‚úÖ **Process Chain Deviations** - Multiple steps skipped or poorly executed

#### How LDIP Would Have Helped

**Query 1:** "Any process conducted by parties that do not fall under Torts Act purview?"

**LDIP Response:**
```
üîç ANALYSIS RESULTS:

1. Dematerialization Process ‚Äî Documented Step Mismatch
   üìÑ Document: Application MA NO 10 OF 2023, Page 3-7
   ‚ö†Ô∏è Finding: Physical-to-demat conversion completed without ownership verification
   üìã Citation: Torts Act Section 12 requires ownership verification
   üîó Related: Benami list reference in Document 5, Page 12

2. NOTIFICATION - MISSING DOCUMENTATION
   üìÑ Document: Missing (expected per Torts Act Section 15)
   ‚ö†Ô∏è Finding: No document evidencing notification sent to Mehta family before share sale
   üìã Citation: Torts Act Section 15 requires 30-day notification
   üîó Related: Sale completed on 2023-09-29, Mehtas notified on 2023-12-15

3. TIMELINE ANOMALY
   üìÑ Documents: Multiple filings across 9-month period
   ‚ö†Ô∏è Finding: Process took 9 months vs typical 2-3 months
   üìã Pattern: Similar process in Kalpana Jobalia case completed in 2 months
   üîó Related: See comparison analysis in Document 8

[All findings require attorney verification]
```

**Query 2:** "Any lapses on custodian's part under Torts Act?"

**LDIP Response:**
```
üîç ANALYSIS RESULTS:

1. OWNERSHIP VERIFICATION ‚Äî DOCUMENT GAP
   üìÑ Document: Custodian Affidavit, Page 5-8
   ‚ö†Ô∏è Finding: No document evidencing ownership verification by custodian was found.
   üìã Citation: Torts Act Section 12(3) requires ownership verification
   üîó Related: Benami list available in company records (Document 3, Page 15)

2. NOTIFICATION - MISSING DOCUMENTATION
   üìÑ Document: Missing notification record
   ‚ö†Ô∏è Finding: No document evidencing notification to interested parties by custodian was found per Section 15
   üìã Citation: Torts Act Section 15(2) requires custodian notification
   üîó Related: Mehta family declared interest in Document 2, Page 3

[All findings require attorney verification]
```

---

## How It Works

### Complete System Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LDIP SYSTEM FLOW                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PHASE 1: DOCUMENT INGESTION
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                               ‚îÇ
‚îÇ  1. Document Upload                                           ‚îÇ
‚îÇ     ‚îî‚îÄ> PDF files uploaded via web interface                 ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  2. Privilege Scanning                                        ‚îÇ
‚îÇ     ‚îî‚îÄ> Detect privilege markers (LOW/MEDIUM/HIGH)           ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  3. Text Extraction                                           ‚îÇ
‚îÇ     ‚îú‚îÄ> Native PDF: Direct extraction                        ‚îÇ
‚îÇ     ‚îú‚îÄ> Scanned PDF: OCR processing                          ‚îÇ
‚îÇ     ‚îî‚îÄ> Low confidence: LLM-assisted extraction              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  4. Metadata Extraction                                       ‚îÇ
‚îÇ     ‚îî‚îÄ> Document type, dates, parties, citations              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  5. Chunking for RAG                                           ‚îÇ
‚îÇ     ‚îî‚îÄ> Split into 400-700 token chunks with overlap         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  6. Vector Embedding                                          ‚îÇ
‚îÇ     ‚îî‚îÄ> Generate embeddings (OpenAI ada-002)                  ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  7. Matter Identity Graph (MIG)                               ‚îÇ
‚îÇ     ‚îî‚îÄ> Pre-link entities, relationships, events              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
PHASE 2: QUERY PROCESSING
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                               ‚îÇ
‚îÇ  1. User Query                                                ‚îÇ
‚îÇ     ‚îî‚îÄ> "Any lapses on custodian's part?"                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  2. Query Orchestrator                                        ‚îÇ
‚îÇ     ‚îú‚îÄ> Parse query intent                                   ‚îÇ
‚îÇ     ‚îú‚îÄ> Determine which engines to activate                  ‚îÇ
‚îÇ     ‚îî‚îÄ> Route to appropriate engines                         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  3. RAG Retrieval                                             ‚îÇ
‚îÇ     ‚îú‚îÄ> Semantic search in vector database                   ‚îÇ
‚îÇ     ‚îú‚îÄ> Filter by matter_id (isolation)                       ‚îÇ
‚îÇ     ‚îî‚îÄ> Retrieve relevant document chunks                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  4. Engine Execution (Parallel)                              ‚îÇ
‚îÇ     ‚îú‚îÄ> Engine 1: Citation Verification                       ‚îÇ
‚îÇ     ‚îú‚îÄ> Engine 2: Timeline Construction                      ‚îÇ
‚îÇ     ‚îú‚îÄ> Engine 3: Consistency & Contradiction                ‚îÇ
‚îÇ     ‚îú‚îÄ> Engine 4: Documentation Gap                           ‚îÇ
‚îÇ     ‚îú‚îÄ> Engine 5: Process Chain Integrity                    ‚îÇ
‚îÇ     ‚îî‚îÄ> Engine 6: Entity Authenticity                         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  5. Evidence Binding                                          ‚îÇ
‚îÇ     ‚îî‚îÄ> Every finding tied to document, page, line            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  6. Response Generation                                        ‚îÇ
‚îÇ     ‚îú‚îÄ> Synthesize engine outputs                            ‚îÇ
‚îÇ     ‚îú‚îÄ> Add citations and confidence scores                   ‚îÇ
‚îÇ     ‚îî‚îÄ> Format for attorney review                           ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
PHASE 3: ATTORNEY REVIEW
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                               ‚îÇ
‚îÇ  1. Review Findings                                           ‚îÇ
‚îÇ     ‚îî‚îÄ> Attorney verifies all findings                        ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  2. Save to Research Journal                                  ‚îÇ
‚îÇ     ‚îî‚îÄ> Personal research notes                              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  3. Take Action                                               ‚îÇ
‚îÇ     ‚îî‚îÄ> Use findings in case strategy                        ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Detailed Flow: Document Upload to Analysis

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           DOCUMENT UPLOAD ‚Üí ANALYSIS PIPELINE                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

USER ACTION
    ‚îÇ
    ‚îú‚îÄ> Upload PDF (e.g., "Affidavit in Reply.pdf")
    ‚îÇ
    ‚Üì
FILE VALIDATION
    ‚îÇ
    ‚îú‚îÄ> Check file type, size, format
    ‚îú‚îÄ> Assign document_id (UUID)
    ‚îú‚îÄ> Store in Supabase Storage
    ‚îÇ   ‚îî‚îÄ> documents-{tenant_id}/{matter_id}/originals/{doc_id}.pdf
    ‚îÇ
    ‚Üì
PRIVILEGE SCANNING
    ‚îÇ
    ‚îú‚îÄ> Scan for privilege markers:
    ‚îÇ   ‚îú‚îÄ> "attorney-client privilege" headers
    ‚îÇ   ‚îú‚îÄ> Counsel email signatures
    ‚îÇ   ‚îî‚îÄ> Strategy discussion keywords
    ‚îÇ
    ‚îú‚îÄ> Assign privilege_score (0-10):
    ‚îÇ   ‚îú‚îÄ> 0-3: LOW ‚Üí Full processing allowed
    ‚îÇ   ‚îú‚îÄ> 4-6: MEDIUM ‚Üí Processing with audit flag
    ‚îÇ   ‚îî‚îÄ> 7-10: HIGH ‚Üí BLOCKED until MatterLead approval
    ‚îÇ
    ‚Üì
TEXT EXTRACTION
    ‚îÇ
    ‚îú‚îÄ> Native PDF?
    ‚îÇ   ‚îî‚îÄ> Direct text extraction
    ‚îÇ
    ‚îú‚îÄ> Scanned PDF?
    ‚îÇ   ‚îú‚îÄ> OCR processing (Tesseract/cloud)
    ‚îÇ   ‚îú‚îÄ> Get OCR confidence per page
    ‚îÇ   ‚îî‚îÄ> If confidence < 70%:
    ‚îÇ       ‚îî‚îÄ> LLM-assisted extraction
    ‚îÇ
    ‚îú‚îÄ> Store text sources:
    ‚îÇ   ‚îú‚îÄ> OCR text (legal record, always preserved)
    ‚îÇ   ‚îî‚îÄ> LLM text (when OCR confidence < 70%)
    ‚îÇ
    ‚Üì
METADATA EXTRACTION
    ‚îÇ
    ‚îú‚îÄ> Extract structured data:
    ‚îÇ   ‚îú‚îÄ> Document type (affidavit, order, application)
    ‚îÇ   ‚îú‚îÄ> Dates mentioned
    ‚îÇ   ‚îú‚îÄ> Parties involved
    ‚îÇ   ‚îú‚îÄ> Acts and sections cited
    ‚îÇ   ‚îî‚îÄ> Financial references (ISINs, amounts)
    ‚îÇ
    ‚îú‚îÄ> Store in documents.metadata (JSONB)
    ‚îÇ
    ‚Üì
CHUNKING FOR RAG
    ‚îÇ
    ‚îú‚îÄ> Split document into chunks:
    ‚îÇ   ‚îú‚îÄ> 400-700 tokens per chunk
    ‚îÇ   ‚îú‚îÄ> Preserve page boundaries
    ‚îÇ   ‚îî‚îÄ> 100-200 word overlap
    ‚îÇ
    ‚îú‚îÄ> Create chunk records:
    ‚îÇ   ‚îú‚îÄ> chunk_id, document_id, matter_id
    ‚îÇ   ‚îú‚îÄ> page_range, text, chunk_index
    ‚îÇ   ‚îî‚îÄ> text_source (OCR/LLM/SELECTED)
    ‚îÇ
    ‚Üì
VECTOR EMBEDDING
    ‚îÇ
    ‚îú‚îÄ> Generate embeddings (OpenAI ada-002, 1536 dimensions)
    ‚îú‚îÄ> Store in Supabase pgvector:
    ‚îÇ   ‚îú‚îÄ> Table: document_embeddings
    ‚îÇ   ‚îú‚îÄ> Namespace: matter_id (isolation)
    ‚îÇ   ‚îî‚îÄ> Metadata: document_id, page_number, chunk_index
    ‚îÇ
    ‚Üì
MATTER IDENTITY GRAPH (MIG) PRE-LINKING
    ‚îÇ
    ‚îú‚îÄ> Extract entities:
    ‚îÇ   ‚îú‚îÄ> Persons (Nirav Jobalia, Jyoti Mehta)
    ‚îÇ   ‚îú‚îÄ> Companies (Hero Honda, Custodian)
    ‚îÇ   ‚îî‚îÄ> Institutions (Court, Registrar)
    ‚îÇ
    ‚îú‚îÄ> Extract relationships:
    ‚îÇ   ‚îú‚îÄ> Nirav Jobalia ‚Üí owns ‚Üí Shares
    ‚îÇ   ‚îú‚îÄ> Mehta family ‚Üí claims ‚Üí Shares
    ‚îÇ   ‚îî‚îÄ> Custodian ‚Üí manages ‚Üí Demat Account
    ‚îÇ
    ‚îú‚îÄ> Extract events:
    ‚îÇ   ‚îú‚îÄ> Share conversion (2023-02-27)
    ‚îÇ   ‚îú‚îÄ> Share sale (2023-09-29)
    ‚îÇ   ‚îî‚îÄ> Notification (missing)
    ‚îÇ
    ‚îú‚îÄ> Store in MIG:
    ‚îÇ   ‚îú‚îÄ> matter_entities table
    ‚îÇ   ‚îú‚îÄ> matter_relationships table
    ‚îÇ   ‚îî‚îÄ> matter_events table
    ‚îÇ
    ‚Üì
READY FOR QUERIES
    ‚îÇ
    ‚îî‚îÄ> Document now searchable and analyzable
```

### Detailed Flow: Query Processing

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              QUERY ‚Üí RESPONSE PIPELINE                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

USER QUERY
    ‚îÇ
    ‚îú‚îÄ> "Any lapses on custodian's part under Torts Act?"
    ‚îÇ
    ‚Üì
QUERY ORCHESTRATOR
    ‚îÇ
    ‚îú‚îÄ> Parse query intent:
    ‚îÇ   ‚îú‚îÄ> Entity: "custodian"
    ‚îÇ   ‚îú‚îÄ> Action: "lapses"
    ‚îÇ   ‚îú‚îÄ> Context: "Torts Act"
    ‚îÇ   ‚îî‚îÄ> Type: Process Chain + Citation Verification
    ‚îÇ
    ‚îú‚îÄ> Determine engine activation:
    ‚îÇ   ‚îú‚îÄ> Engine 5: Process Chain Integrity ‚úì
    ‚îÇ   ‚îú‚îÄ> Engine 1: Citation Verification ‚úì
    ‚îÇ   ‚îú‚îÄ> Engine 4: Documentation Gap ‚úì
    ‚îÇ   ‚îî‚îÄ> Engine 3: Consistency & Contradiction ‚úì
    ‚îÇ
    ‚Üì
RAG RETRIEVAL
    ‚îÇ
    ‚îú‚îÄ> Semantic search in vector database:
    ‚îÇ   ‚îú‚îÄ> Query embedding generated
    ‚îÇ   ‚îú‚îÄ> Similarity search (cosine distance)
    ‚îÇ   ‚îú‚îÄ> Filter by matter_id (isolation)
    ‚îÇ   ‚îî‚îÄ> Retrieve top 20-30 relevant chunks
    ‚îÇ
    ‚îú‚îÄ> Retrieve from MIG:
    ‚îÇ   ‚îú‚îÄ> All entities matching "custodian"
    ‚îÇ   ‚îú‚îÄ> All relationships involving custodian
    ‚îÇ   ‚îî‚îÄ> All events involving custodian
    ‚îÇ
    ‚Üì
ENGINE EXECUTION (Parallel)
    ‚îÇ
    ‚îú‚îÄ> Engine 5: Process Chain Integrity
    ‚îÇ   ‚îú‚îÄ> Query Act Knowledge Base for expected process steps
    ‚îÇ   ‚îÇ   ‚îî‚îÄ> Uses pre-defined process templates (e.g., Dematerialization)
   ‚îÇ   ‚îú‚îÄ> Query events table for actual steps performed
   ‚îÇ   ‚îú‚îÄ> Compare expected vs actual process steps
   ‚îÇ   ‚îú‚îÄ> Identify missing steps, timeline deviations relative to the template, out-of-order steps
   ‚îÇ   ‚îî‚îÄ> Output: List of lapses with citations
    ‚îÇ
    ‚îú‚îÄ> Engine 1: Citation Verification
    ‚îÇ   ‚îú‚îÄ> Verify Torts Act citations in documents
    ‚îÇ   ‚îú‚îÄ> Check for misquotations
    ‚îÇ   ‚îî‚îÄ> Output: Citation accuracy report
    ‚îÇ
    ‚îú‚îÄ> Engine 4: Documentation Gap
    ‚îÇ   ‚îú‚îÄ> Check for missing required documents
    ‚îÇ   ‚îú‚îÄ> Identify expected but absent records
    ‚îÇ   ‚îî‚îÄ> Output: Missing documentation list
    ‚îÇ
    ‚îú‚îÄ> Engine 3: Consistency & Contradiction
    ‚îÇ   ‚îú‚îÄ> Compare custodian statements across documents
    ‚îÇ   ‚îú‚îÄ> Identify contradictions
    ‚îÇ   ‚îî‚îÄ> Output: Inconsistency report
    ‚îÇ
    ‚Üì
EVIDENCE BINDING
    ‚îÇ
    ‚îú‚îÄ> Every finding must have:
    ‚îÇ   ‚îú‚îÄ> Source document_id
    ‚îÇ   ‚îú‚îÄ> Page number
    ‚îÇ   ‚îú‚îÄ> Line number (if available)
    ‚îÇ   ‚îú‚îÄ> Text excerpt
    ‚îÇ   ‚îî‚îÄ> Confidence score
    ‚îÇ
    ‚îú‚îÄ> If evidence missing:
    ‚îÇ   ‚îî‚îÄ> Mark as "Not determinable from provided materials"
    ‚îÇ
    ‚Üì
RESPONSE SYNTHESIS
    ‚îÇ
    ‚îú‚îÄ> Combine engine outputs:
    ‚îÇ   ‚îú‚îÄ> Remove duplicates
    ‚îÇ   ‚îú‚îÄ> Rank by confidence
    ‚îÇ   ‚îî‚îÄ> Group by category
    ‚îÇ
    ‚îú‚îÄ> Format response:
    ‚îÇ   ‚îú‚îÄ> Executive summary
    ‚îÇ   ‚îú‚îÄ> Detailed findings with citations
    ‚îÇ   ‚îú‚îÄ> Confidence scores
    ‚îÇ   ‚îî‚îÄ> "Requires attorney verification" disclaimer
    ‚îÇ
    ‚Üì
RESPONSE TO USER
    ‚îÇ
    ‚îî‚îÄ> Formatted analysis with all citations
```

---

## Key Features

### Eight Specialized Detection Engines

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LDIP DETECTION ENGINES                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

ENGINE 1: CITATION VERIFICATION
‚îú‚îÄ Purpose: Verify Act citations are accurate and complete
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Misquotations from Acts
‚îÇ   ‚îú‚îÄ Omitted provisos or conflicting sections
‚îÇ   ‚îî‚îÄ Incomplete citations
‚îî‚îÄ Example: "Document claims Section 12 says X, but actual text says Y"

ENGINE 2: TIMELINE CONSTRUCTION
‚îú‚îÄ Purpose: Reconstruct chronological sequence of events
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Timeline anomalies (unusual durations)
‚îÇ   ‚îú‚îÄ Out-of-order events
‚îÇ   ‚îú‚îÄ Missing timeline segments
‚îÇ   ‚îî‚îÄ Silence, delay & absence intelligence
‚îî‚îÄ Example: "Process took 9 months vs typical 2-3 months"

ENGINE 3: CONSISTENCY & CONTRADICTION
‚îú‚îÄ Purpose: Find inconsistencies across documents
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Contradictory statements
‚îÇ   ‚îú‚îÄ Conflicting narratives
‚îÇ   ‚îî‚îÄ Inconsistent facts
‚îî‚îÄ Example: "Party A claims X in Document 1, but Y in Document 5"

ENGINE 4: DOCUMENTATION GAP
‚îú‚îÄ Purpose: Identify missing required documents
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Expected but absent documents
‚îÇ   ‚îú‚îÄ Missing procedural steps
‚îÇ   ‚îî‚îÄ Incomplete documentation chains
‚îî‚îÄ Example: "Notification required per Section 15, but no record found"

ENGINE 5: PROCESS CHAIN INTEGRITY
‚îú‚îÄ Purpose: Compare documented actions against pre-defined institutional process templates.
‚îú‚îÄ How it works:
‚îÇ   ‚îú‚îÄ Queries Act Knowledge Base for pre-defined process templates
‚îÇ   ‚îú‚îÄ Uses domain-specific templates (demat, company law, employment, etc.)
‚îÇ   ‚îú‚îÄ Compares expected steps (from template) vs actual steps (from documents)
‚îÇ   ‚îî‚îÄ No web searching - uses structured Act database + process templates
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Skipped required steps
‚îÇ   ‚îú‚îÄ Out-of-order processes
‚îÇ   ‚îú‚îÄ Timeline deviations relative to the template
‚îÇ   ‚îî‚îÄ Missing expected steps
‚îî‚îÄ Example: "Demat conversion completed without ownership verification"

ENGINE 6: ENTITY AUTHENTICITY
‚îú‚îÄ Purpose: Verify entity claims and relationships
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Identity mismatches
‚îÇ   ‚îú‚îÄ Recorded ownership inconsistencies across documents
‚îÇ   ‚îî‚îÄ Role inconsistencies
‚îî‚îÄ Example: "Shares claimed by Party A, but benami list shows Party B"

ENGINE 7: ADMISSIONS & NON-DENIAL DETECTOR
‚îú‚îÄ Purpose: Flag explicit admissions, partial admissions, and non-denial patterns
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Explicit admissions
‚îÇ   ‚îú‚îÄ Partial admissions
‚îÇ   ‚îú‚îÄ "Para denied for want of knowledge" patterns
‚îÇ   ‚îî‚îÄ Silent non-denials
‚îî‚îÄ Example: "Party A admitted [fact] in Document X, page Y"

ENGINE 8: PLEADING-VS-DOCUMENT MISMATCH
‚îú‚îÄ Purpose: Detect when pleadings claim X but documents only support Y
‚îú‚îÄ Detects:
‚îÇ   ‚îú‚îÄ Over-broad legal claims backed by narrow facts
‚îÇ   ‚îú‚îÄ Annexures that don't support pleading claims
‚îÇ   ‚îî‚îÄ Pleading-document disconnects
‚îî‚îÄ Example: "Pleading claims X but supporting document only shows Y"
```

### Matter Isolation & Security

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           MATTER ISOLATION ARCHITECTURE                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

TENANT LEVEL
    ‚îÇ
    ‚îú‚îÄ> Law Firm A
    ‚îÇ   ‚îú‚îÄ> Matter 1: Case XYZ (isolated)
    ‚îÇ   ‚îú‚îÄ> Matter 2: Case ABC (isolated)
    ‚îÇ   ‚îî‚îÄ> Matter 3: Case DEF (isolated)
    ‚îÇ
    ‚îî‚îÄ> Law Firm B
        ‚îú‚îÄ> Matter 4: Case GHI (isolated)
        ‚îî‚îÄ> Matter 5: Case JKL (isolated)

ISOLATION ENFORCEMENT
    ‚îÇ
    ‚îú‚îÄ> Database: All queries filtered by matter_id
    ‚îú‚îÄ> Vector Search: Namespace = matter_id
    ‚îú‚îÄ> Storage: Folders organized by matter_id
    ‚îú‚îÄ> Access Control: Role-based permissions per matter
    ‚îî‚îÄ> Audit Trail: All access logged per matter

CROSS-MATTER ACCESS
    ‚îÇ
    ‚îú‚îÄ> Phase 1 (MVP): BLOCKED (strict isolation)
    ‚îî‚îÄ> Phase 2+: Allowed only with explicit authorization
        ‚îî‚îÄ> Requires MatterLead approval + conflict check
```

### Evidence-First Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           EVIDENCE BINDING REQUIREMENTS                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

EVERY FINDING MUST INCLUDE:

1. SOURCE DOCUMENT
   ‚îú‚îÄ> document_id (UUID)
   ‚îú‚îÄ> file_name
   ‚îî‚îÄ> document_type

2. LOCATION
   ‚îú‚îÄ> page_number
   ‚îú‚îÄ> line_number (if available)
   ‚îî‚îÄ> text_source (OCR/LLM/SELECTED)

3. TEXT EXCERPT
   ‚îú‚îÄ> Exact quote from document
   ‚îú‚îÄ> Context (surrounding text)
   ‚îî‚îÄ> Character offsets

4. CONFIDENCE SCORE
   ‚îú‚îÄ> HIGH (90-100%): Strong evidence
   ‚îú‚îÄ> MEDIUM (70-89%): Moderate evidence
   ‚îî‚îÄ> LOW (50-69%): Weak evidence, requires verification

5. UNCERTAINTY LABELS
   ‚îú‚îÄ> "Determined from provided materials"
   ‚îú‚îÄ> "Not determinable from provided materials"
   ‚îî‚îÄ> "Requires additional documents"

EXAMPLE:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Finding: No document evidencing ownership verification by   ‚îÇ
‚îÇ          custodian was found.                                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ Source:                                                      ‚îÇ
‚îÇ   Document: "Affidavit in Reply.pdf"                         ‚îÇ
‚îÇ   Page: 5-8                                                 ‚îÇ
‚îÇ   Line: 45-67                                                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ Text Excerpt:                                                ‚îÇ
‚îÇ   "The custodian processed the dematerialization request     ‚îÇ
‚îÇ   without requiring ownership verification documents..."      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ Citation: Torts Act Section 12(3) requires ownership        ‚îÇ
‚îÇ verification before dematerialization                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ Confidence: HIGH (95%)                                      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ Status: Requires attorney verification                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Technical Architecture

### System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              LDIP SYSTEM ARCHITECTURE                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

FRONTEND LAYER
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Web Application (React/Next.js)                            ‚îÇ
‚îÇ  ‚îú‚îÄ> Document Upload Interface                              ‚îÇ
‚îÇ  ‚îú‚îÄ> Query Interface                                         ‚îÇ
‚îÇ  ‚îú‚îÄ> Results Dashboard                                       ‚îÇ
‚îÇ  ‚îî‚îÄ> Research Journal                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
API LAYER
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  REST API (Node.js/Express)                                 ‚îÇ
‚îÇ  ‚îú‚îÄ> Authentication & Authorization                          ‚îÇ
‚îÇ  ‚îú‚îÄ> Document Management API                                 ‚îÇ
‚îÇ  ‚îú‚îÄ> Query Processing API                                    ‚îÇ
‚îÇ  ‚îî‚îÄ> Matter Management API                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
BUSINESS LOGIC LAYER
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Query Orchestrator                                          ‚îÇ
‚îÇ  ‚îú‚îÄ> Query Intent Parser                                     ‚îÇ
‚îÇ  ‚îú‚îÄ> Engine Router                                           ‚îÇ
‚îÇ  ‚îî‚îÄ> Response Synthesizer                                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Detection Engines (8 engines)                              ‚îÇ
‚îÇ  ‚îú‚îÄ> Citation Verification Engine                           ‚îÇ
‚îÇ  ‚îú‚îÄ> Timeline Construction Engine                            ‚îÇ
‚îÇ  ‚îú‚îÄ> Consistency & Contradiction Engine                       ‚îÇ
‚îÇ  ‚îú‚îÄ> Documentation Gap Engine                                ‚îÇ
‚îÇ  ‚îú‚îÄ> Process Chain Integrity Engine                          ‚îÇ
‚îÇ  ‚îú‚îÄ> Entity Authenticity Engine                              ‚îÇ
‚îÇ  ‚îú‚îÄ> Admissions & Non-Denial Detector                         ‚îÇ
‚îÇ  ‚îî‚îÄ> Pleading-vs-Document Mismatch Engine                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Document Processing Pipeline                                ‚îÇ
‚îÇ  ‚îú‚îÄ> Privilege Scanner                                       ‚îÇ
‚îÇ  ‚îú‚îÄ> Text Extractor (OCR/LLM)                               ‚îÇ
‚îÇ  ‚îú‚îÄ> Metadata Extractor                                     ‚îÇ
‚îÇ  ‚îú‚îÄ> Chunker                                                ‚îÇ
‚îÇ  ‚îî‚îÄ> Embedding Generator                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
DATA LAYER
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Supabase (PostgreSQL + Storage)                            ‚îÇ
‚îÇ  ‚îú‚îÄ> PostgreSQL Database                                     ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ> matters table                                      ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ> documents table                                    ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ> document_chunks table                              ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ> matter_entities table (MIG)                        ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ> matter_relationships table (MIG)                   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ> matter_events table (MIG)                         ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ
‚îÇ  ‚îú‚îÄ> pgvector Extension                                     ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ> document_embeddings table                          ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ
‚îÇ  ‚îî‚îÄ> Supabase Storage                                       ‚îÇ
‚îÇ      ‚îú‚îÄ> Original PDFs                                      ‚îÇ
‚îÇ      ‚îú‚îÄ> OCR text files                                     ‚îÇ
‚îÇ      ‚îî‚îÄ> LLM-extracted text files                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  External Services                                          ‚îÇ
‚îÇ  ‚îú‚îÄ> OpenAI API (Embeddings, LLM)                           ‚îÇ
‚îÇ  ‚îú‚îÄ> OCR Service (Tesseract/Cloud)                           ‚îÇ
‚îÇ  ‚îî‚îÄ> Act Knowledge Base (Pre-defined process templates)     ‚îÇ
‚îÇ      ‚îú‚îÄ> Structured Acts (Torts Act, etc.)                  ‚îÇ
‚îÇ      ‚îú‚îÄ> Process templates (Dematerialization, etc.)        ‚îÇ
‚îÇ      ‚îî‚îÄ> Stored in PostgreSQL (acts, sections, templates)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technology Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TECHNOLOGY STACK                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

FRONTEND
‚îú‚îÄ> Framework: React / Next.js
‚îú‚îÄ> UI Library: Tailwind CSS / shadcn/ui
‚îî‚îÄ> State Management: React Query / Zustand

BACKEND
‚îú‚îÄ> Runtime: Node.js
‚îú‚îÄ> Framework: Express.js / Fastify
‚îú‚îÄ> Language: TypeScript
‚îî‚îÄ> API: REST (GraphQL in Phase 2)

DATABASE
‚îú‚îÄ> Primary: PostgreSQL (Supabase)
‚îú‚îÄ> Vector Search: pgvector extension
‚îú‚îÄ> Storage: Supabase Storage (S3-compatible)
‚îî‚îÄ> Caching: Redis (Phase 2)

AI/ML
‚îú‚îÄ> Embeddings: OpenAI ada-002 (1536 dimensions)
‚îú‚îÄ> LLM: OpenAI GPT-4 / Claude (for analysis)
‚îú‚îÄ> OCR: Tesseract / Google Cloud Vision
‚îî‚îÄ> RAG: Custom implementation with pgvector

INFRASTRUCTURE
‚îú‚îÄ> Hosting: Vercel / AWS / Supabase
‚îú‚îÄ> CI/CD: GitHub Actions
‚îú‚îÄ> Monitoring: Sentry / DataDog
‚îî‚îÄ> Logging: Winston / Pino
```

---

## Value Proposition

### For Junior Associates

**Time Savings:**
- ‚è±Ô∏è **70% reduction** in document analysis time
- ‚è±Ô∏è **30 minutes** to orient to new matter (vs. hours/days)
- ‚è±Ô∏è **2 hours** to identify all gaps (vs. days)

**Quality Improvements:**
- ‚úÖ Catch issues they would have missed
- ‚úÖ Systematic approach to finding contradictions
- ‚úÖ Automatic citation verification
- ‚úÖ Complete timeline reconstruction

### For Senior Lawyers/Partners

**Validation & Quality:**
- ‚úÖ **85%+ accuracy** vs. manual review
- ‚úÖ Validate junior research findings quickly
- ‚úÖ Scan for subtle contradictions across hundreds of documents
- ‚úÖ Cross-check factual assumptions consistently

**Strategic Insights:**
- ‚úÖ Identify patterns across multiple matters (when authorized)
- ‚úÖ Discover connections that would take weeks manually
- ‚úÖ Surface anomalies requiring investigation

### For Law Firms

**Business Value:**
- üí∞ **40+ hours saved** per matter
- üí∞ **10x faster** document analysis
- üí∞ **Better case outcomes** through comprehensive analysis
- üí∞ **Reduced malpractice risk** from missed issues

**Competitive Advantage:**
- üöÄ First-mover advantage in AI-assisted legal analysis
- üöÄ Higher quality case preparation
- üöÄ Better client satisfaction
- üöÄ Attract top talent with cutting-edge tools

### ROI Calculation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ROI ANALYSIS                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

COST SAVINGS PER MATTER:

Manual Analysis:
‚îú‚îÄ> Junior Associate: 50-85 hours @ $100/hour = $5,000-$8,500
‚îú‚îÄ> Senior Review: 10-15 hours @ $300/hour = $3,000-$4,500
‚îî‚îÄ> Total: $8,000-$13,000 per matter

With LDIP:
‚îú‚îÄ> Junior Associate: 15-25 hours @ $100/hour = $1,500-$2,500
‚îú‚îÄ> Senior Review: 3-5 hours @ $300/hour = $900-$1,500
‚îú‚îÄ> LDIP Subscription: ~$500 per matter
‚îî‚îÄ> Total: $2,900-$4,500 per matter

SAVINGS: $5,100-$8,500 per matter (40-65% reduction)

ANNUAL ROI (100 matters):
‚îú‚îÄ> Cost Savings: $510,000-$850,000
‚îú‚îÄ> LDIP Cost: $50,000
‚îî‚îÄ> Net Savings: $460,000-$800,000

PAYBACK PERIOD: <1 month
```

---

## Next Steps

### Implementation Roadmap

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PHASED IMPLEMENTATION PLAN                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PHASE 1: MVP (Months 1-3)
‚îú‚îÄ> Core Features:
‚îÇ   ‚îú‚îÄ> Document upload and processing
‚îÇ   ‚îú‚îÄ> Basic privilege scanning
‚îÇ   ‚îú‚îÄ> Text extraction (OCR + LLM)
‚îÇ   ‚îú‚îÄ> Vector embedding and RAG
‚îÇ   ‚îú‚îÄ> 3 Detection Engines (Citation, Timeline, Consistency)
‚îÇ   ‚îú‚îÄ> Matter isolation
‚îÇ   ‚îî‚îÄ> Basic query interface
‚îÇ
‚îú‚îÄ> Success Criteria:
‚îÇ   ‚îú‚îÄ> 10+ law firms using platform
‚îÇ   ‚îú‚îÄ> 50+ matters processed
‚îÇ   ‚îú‚îÄ> 80%+ user satisfaction
‚îÇ   ‚îî‚îÄ> Zero privilege breaches
‚îÇ
‚îî‚îÄ> Deliverables:
    ‚îú‚îÄ> Working MVP
    ‚îú‚îÄ> User documentation
    ‚îî‚îÄ> Training materials

PHASE 2: Enhanced Capabilities (Months 4-6)
‚îú‚îÄ> Additional Features:
‚îÇ   ‚îú‚îÄ> All 6 Detection Engines
‚îÇ   ‚îú‚îÄ> Advanced MIG pre-linking
‚îÇ   ‚îú‚îÄ> Research Journal
‚îÇ   ‚îú‚îÄ> Multi-user collaboration
‚îÇ   ‚îî‚îÄ> Performance optimizations
‚îÇ
‚îú‚îÄ> Success Criteria:
‚îÇ   ‚îú‚îÄ> 100+ matters processed
‚îÇ   ‚îú‚îÄ> 85%+ timeline accuracy
‚îÇ   ‚îú‚îÄ> <3 minute query response time
‚îÇ   ‚îî‚îÄ> 90%+ citation verification accuracy
‚îÇ
‚îî‚îÄ> Deliverables:
    ‚îú‚îÄ> Enhanced platform
    ‚îú‚îÄ> Advanced features
    ‚îî‚îÄ> Performance improvements

PHASE 3: Advanced Features (Months 7-9)
‚îú‚îÄ> Advanced Features:
‚îÇ   ‚îú‚îÄ> Cross-matter analysis (with authorization)
‚îÇ   ‚îú‚îÄ> Pattern discovery
‚îÇ   ‚îú‚îÄ> Predictive document gaps
‚îÇ   ‚îî‚îÄ> Learning from corrections
‚îÇ
‚îú‚îÄ> Success Criteria:
‚îÇ   ‚îú‚îÄ> 1,000+ matters processed
‚îÇ   ‚îú‚îÄ> 95%+ accuracy across all engines
‚îÇ   ‚îî‚îÄ> Positive ROI demonstrated
‚îÇ
‚îî‚îÄ> Deliverables:
    ‚îú‚îÄ> Full-featured platform
    ‚îú‚îÄ> Advanced analytics
    ‚îî‚îÄ> Case studies
```

### Getting Started

**For Development Team:**
1. Review technical architecture document
2. Set up development environment
3. Begin Phase 1 MVP implementation
4. Start with document upload and processing pipeline

**For Product Team:**
1. Review PRD and user requirements
2. Design user interface mockups
3. Create user training materials
4. Plan pilot program with law firms

**For Stakeholders:**
1. Review this pitch document
2. Approve Phase 1 implementation plan
3. Allocate resources
4. Set success metrics

---

## How Process Workflows Work

### Pre-Defined Process Templates (Not Web Search)

**Important:** LDIP does NOT web search for process workflows. Instead, it uses:

1. **Act Knowledge Base** - A structured database containing:
   - Complete legal Acts (Torts Act, etc.)
   - Sections, subsections, provisos, explanations
   - Stored in PostgreSQL tables: `acts`, `sections`, `section_components`

2. **Pre-Defined Process Templates** - Based on Act requirements:
   - Process templates are manually created from Act provisions
   - Each template defines required steps, timelines, responsible parties
   - Example: "Dematerialization Process" template with 6 required steps

### Example: Dematerialization Process Template

```
Process: Dematerialization of Physical Shares
Authority: Torts Act Section 12
Expected Duration: 60-90 days

Required Steps (MUST occur):
1. Written Request Submitted (Section 12(1))
   - Documents: Demat request form, Share certificates
   - Responsible: Shareholder
   
2. Ownership Verification (Section 12(2)(a))
   - Documents: Payment proof, Chain of title
   - Responsible: Custodian
   
3. Custodian Approval (Section 12(3))
   - Documents: Approval letter, Custodian signature
   - Responsible: Custodian
   - Conditions: No objections, Verification complete
   
4. Dematerialization Executed
   - Documents: Demat confirmation, Updated records
   - Responsible: Depository

Optional Steps (CAN occur):
1. Notification to Interested Parties (Section 15(2))
   - Documents: Notification letter, Postal receipts
   - Responsible: Custodian
   - Note: Required only if objections are raised
   
2. Waiting Period (Section 15(3))
   - Duration: Minimum 7 days
   - Purpose: Allow objections
   - Note: May be waived if no objections

Order Flexible Steps (Order doesn't matter):
1. Additional Documentation Review
2. Third-Party Verification (if required)

Timing Constraints:
- Step 1 (Request) to Step 2 (Verification): < 30 days
- Step 2 (Verification) to Step 3 (Approval): < 60 days
- Step 3 (Approval) to Step 4 (Execution): < 14 days
```

### How Process Chain Engine Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         PROCESS CHAIN VERIFICATION FLOW                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. USER QUERY
   ‚îî‚îÄ> "Any lapses in dematerialization process?"

2. PROCESS CHAIN ENGINE ACTIVATED
   ‚îÇ
   ‚îú‚îÄ> Query Act Knowledge Base:
   ‚îÇ   ‚îú‚îÄ> Retrieve "Dematerialization Process" template
   ‚îÇ   ‚îú‚îÄ> Get expected steps from template
   ‚îÇ   ‚îî‚îÄ> Get timeline requirements from Act sections
   ‚îÇ
   ‚îú‚îÄ> Query Matter Documents:
   ‚îÇ   ‚îú‚îÄ> Search for evidence of each required step
   ‚îÇ   ‚îú‚îÄ> Extract actual events from documents
   ‚îÇ   ‚îî‚îÄ> Identify responsible parties and dates
   ‚îÇ
   ‚îú‚îÄ> Compare Expected vs Actual (Composite Template Matching):
   ‚îÇ   ‚îú‚îÄ> Check required_steps (strict):
   ‚îÇ   ‚îÇ   ‚îú‚îÄ> Missing required steps? ‚Üí Flag as CRITICAL
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Wrong party acted on required step? ‚Üí Flag as CRITICAL
   ‚îÇ   ‚îú‚îÄ> Check optional_steps (flexible):
   ‚îÇ   ‚îú‚îÄ> Check order_flexible steps (order-independent):
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Allow flexible ordering on optional steps
   ‚îÇ   ‚îú‚îÄ> Check timing_constraints:
   ‚îÇ   ‚îÇ   ‚îú‚îÄ> Timeline deviations relative to constraints? ‚Üí Flag with confidence
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Flag timing deviations with confidence score
   ‚îÇ   ‚îî‚îÄ> Missing documents? ‚Üí Flag as MEDIUM
   ‚îÇ
   ‚îî‚îÄ> Generate Report:
       ‚îú‚îÄ> Step-by-step verification
       ‚îú‚îÄ> Deviations with severity and confidence scores
       ‚îî‚îÄ> Citations to Act sections

3. OUTPUT EXAMPLE:
   ‚úì Step 1: Written Request Submitted (COMPLETED)
   ‚úó Step 2: Ownership Verification (MISSING - CRITICAL)
      Confidence this is anomalous: 92%
      Reasoning: Required step per template. Missing in only 2% of authorized matters.
      See evidence: Page 5, Line 12.
   ‚ö†Ô∏è Step 3: Notification to Interested Parties (OPTIONAL - NOT FOUND)
      Confidence this is anomalous: 45%
      Reasoning: Optional step per template. Present in 60% of similar cases.
   ‚úì Step 4: Waiting Period (COMPLETED - but excessive delay)
      Timing deviation: 45 days vs expected < 7 days
      Confidence this is anomalous: 78%
      Reasoning: Timing constraint violation. Only 15% of cases exceed 7 days.
   ‚ö†Ô∏è Step 5: Custodian Approval (COMPLETED BUT IMPROPER)
      Confidence this is anomalous: 85%
      Reasoning: Required step completed but missing prerequisite (Step 2).
   ‚úì Step 6: Dematerialization Executed (COMPLETED)
```

### What LDIP Means by "Process"

In LDIP, a "process" refers to a repeatable, institutionally enforced execution workflow that occurs outside the courtroom and can be blocked or unblocked by documents.

Processes are pre-defined and versioned by the LDIP team.

User documents never create new processes; they only activate, block, or contextualize existing ones.

### Phase 1 (MVP) vs Phase 1.5 vs Phase 2 vs Phase 3

**Phase 1 (MVP): Pre-Defined Templates (Months 1-3)**
- 5-8 core templates (demat, custodian, company law, etc.)
- Process templates manually created from Act requirements
- Templates stored in Act Knowledge Base with composite structure (required/optional/flexible-order steps)
- System compares documents against these templates
- **No web searching** - all knowledge from Acts and templates

**Phase 1.5 (Months 3-4): Strategic Template Expansion**
- Analyze actual user queries and document patterns (not guesses)
- Review usage data from first 3 months
- Add 2-3 new templates for high-frequency case types (only if 10%+ of cases need them)
- Template team reviews data quarterly (not continuous)
- Cost: Template team reviews data quarterly
- Benefit: Accuracy stays high. Templates expand only when needed.

**Phase 2 (Post-MVP): Enhanced Capabilities (Months 4-6)**
- Add 2-3 more templates as new patterns emerge (data-driven)
- Add confidence scoring to all findings output
- LDIP does not learn or generate new process templates. It may learn aggregate statistics such as typical durations, common blockages, and frequency of missing documents.
- Can identify process patterns across multiple matters
- Still validates against Act requirements
- **Still no web searching** - learns from case data
- Quarterly template review process continues (analyze 1000 queries every 3 months)

**Phase 3 (Months 7-9): Bounded Adaptive Computation**
- By Phase 3, you have enough data
- Then you can safely use fuzzy matching with learned baselines
- But by then, template overhead is solved anyway through composite structure
- Bounded adaptive computation for novel pattern discovery

### Act Knowledge Base Structure

```
Act Knowledge Base (PostgreSQL)
‚îú‚îÄ> acts table
‚îÇ   ‚îú‚îÄ> act_id (primary key)
‚îÇ   ‚îú‚îÄ> act_name (e.g., "Torts Act")
‚îÇ   ‚îú‚îÄ> act_year (e.g., 1992)
‚îÇ   ‚îú‚îÄ> jurisdiction (e.g., "India", "UK", "US-Federal")
‚îÇ   ‚îú‚îÄ> effective_date (when Act came into force)
‚îÇ   ‚îú‚îÄ> amendment_date (last amendment, if any)
‚îÇ   ‚îî‚îÄ> full_text
‚îÇ
‚îú‚îÄ> sections table
‚îÇ   ‚îú‚îÄ> section_id (primary key)
‚îÇ   ‚îú‚îÄ> act_id (foreign key ‚Üí acts)
‚îÇ   ‚îú‚îÄ> section_number (e.g., "12")
‚îÇ   ‚îú‚îÄ> section_text
‚îÇ   ‚îî‚îÄ> hierarchy (Part/Chapter/Section)
‚îÇ
‚îú‚îÄ> process_templates table
‚îÇ   ‚îú‚îÄ> template_id (primary key)
‚îÇ   ‚îú‚îÄ> process_name (e.g., "Dematerialization")
‚îÇ   ‚îú‚îÄ> act_id (foreign key ‚Üí acts)
‚îÇ   ‚îú‚îÄ> authority_sections (JSONB: ["12", "15"])
‚îÇ   ‚îú‚îÄ> jurisdiction (e.g., "India")
‚îÇ   ‚îú‚îÄ> applicable_years (JSONB: {"start": 1992, "end": null})
‚îÇ   ‚îú‚îÄ> required_steps (JSONB array) - MUST occur (strict check)
‚îÇ   ‚îú‚îÄ> optional_steps (JSONB array) - CAN occur (flexible check)
‚îÇ   ‚îú‚îÄ> order_flexible (JSONB array) - Order doesn't matter
‚îÇ   ‚îú‚îÄ> timing_constraints (JSONB object) - Step-to-step timing requirements
‚îÇ   ‚îî‚îÄ> validation_checks (JSONB array)
‚îÇ
‚îî‚îÄ> section_components table
    ‚îú‚îÄ> component_id (primary key)
    ‚îú‚îÄ> section_id (foreign key ‚Üí sections)
    ‚îú‚îÄ> component_type (PROVISO/EXPLANATION/ILLUSTRATION)
    ‚îî‚îÄ> component_text
```

### Matter Metadata Structure

```
matters table (PostgreSQL)
‚îú‚îÄ> matter_id (primary key)
‚îú‚îÄ> matter_name
‚îú‚îÄ> jurisdiction (e.g., "India", "Maharashtra", "Delhi High Court")
‚îú‚îÄ> court_name (e.g., "Bombay High Court", "Supreme Court of India")
‚îú‚îÄ> court_type (e.g., "High Court", "District Court", "Tribunal")
‚îú‚îÄ> case_number
‚îú‚îÄ> case_year (e.g., 2023)
‚îú‚îÄ> applicable_acts (JSONB array: ["Torts Act 1992", "Companies Act 2013"])
‚îú‚îÄ> matter_created_at
‚îî‚îÄ> metadata (JSONB: additional case details)
```

### How Template Selection Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         TEMPLATE IDENTIFICATION & SELECTION FLOW             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

STEP 1: MATTER CREATION (Attorney Input)
    ‚îÇ
    ‚îú‚îÄ> Attorney creates matter and provides:
    ‚îÇ   ‚îú‚îÄ> Jurisdiction (e.g., "India", "Maharashtra")
    ‚îÇ   ‚îú‚îÄ> Court (e.g., "Bombay High Court")
    ‚îÇ   ‚îú‚îÄ> Case year (e.g., 2023)
    ‚îÇ   ‚îî‚îÄ> Applicable Acts (e.g., ["Torts Act 1992"])
    ‚îÇ
    ‚îî‚îÄ> Stored in matters table

STEP 2: DOCUMENT UPLOAD & ANALYSIS
    ‚îÇ
    ‚îú‚îÄ> System extracts from documents:
    ‚îÇ   ‚îú‚îÄ> Act citations (e.g., "Torts Act Section 12")
    ‚îÇ   ‚îú‚îÄ> Process mentions (e.g., "dematerialization", "share conversion")
    ‚îÇ   ‚îú‚îÄ> Dates (to determine applicable Act version)
    ‚îÇ   ‚îî‚îÄ> Court/jurisdiction references
    ‚îÇ
    ‚îî‚îÄ> Stored in documents.metadata (JSONB)

STEP 3: TEMPLATE IDENTIFICATION (When Process Chain Engine Runs)
    ‚îÇ
    ‚îú‚îÄ> Query: "Any lapses in dematerialization process?"
    ‚îÇ
    ‚îú‚îÄ> Process Chain Engine:
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ> 1. Get Matter Context:
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> Query matters table for matter_id
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> Get jurisdiction, court, case_year
    ‚îÇ   ‚îÇ   ‚îî‚îÄ> Get applicable_acts array
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ> 2. Identify Process Type:
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> Parse query: "dematerialization process"
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> OR extract from documents: mentions of "demat", "dematerialization"
    ‚îÇ   ‚îÇ   ‚îî‚îÄ> Map to process_name: "Dematerialization"
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ> 3. Find Matching Templates:
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> Query process_templates table:
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ> WHERE process_name = "Dematerialization"
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ> AND jurisdiction = matter.jurisdiction
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ> AND act_id IN (SELECT act_id FROM acts 
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ                  WHERE act_name IN matter.applicable_acts
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ                  AND act_year <= matter.case_year)
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ> AND (applicable_years->>'start' <= matter.case_year
    ‚îÇ   ‚îÇ   ‚îÇ       AND (applicable_years->>'end' IS NULL 
    ‚îÇ   ‚îÇ   ‚îÇ            OR applicable_years->>'end' >= matter.case_year))
    ‚îÇ   ‚îÇ   ‚îÇ
    ‚îÇ   ‚îÇ   ‚îî‚îÄ> Result: Matching template(s) for jurisdiction + year
    ‚îÇ   ‚îÇ
   ‚îÇ   ‚îú‚îÄ> 4. Handle Multiple Templates:
   ‚îÇ   ‚îÇ   ‚îú‚îÄ> If multiple templates found:
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ> Prefer the template version in force at the time the relevant real-world action should have occurred.
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ> Prefer most specific jurisdiction
   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Present options to attorney if ambiguous
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> If no template found:
   ‚îÇ   ‚îÇ       ‚îî‚îÄ> Return: "No process template available for [jurisdiction] [year]"
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îî‚îÄ> 5. Use Selected Template (Composite Template Matching):
    ‚îÇ       ‚îú‚îÄ> Load required_steps from template (strict check - MUST occur)
    ‚îÇ       ‚îú‚îÄ> Load optional_steps from template (flexible check - CAN occur)
    ‚îÇ       ‚îú‚îÄ> Load order_flexible steps from template (order-independent check)
    ‚îÇ       ‚îú‚îÄ> Load timing_constraints from template (timing deviation check)
    ‚îÇ       ‚îú‚îÄ> Load Act sections from acts table
    ‚îÇ       ‚îî‚îÄ> Compare against actual events in documents:
    ‚îÇ           ‚îú‚îÄ> Check required steps (strict)
    ‚îÇ           ‚îú‚îÄ> Allow flexible ordering on optional steps
    ‚îÇ           ‚îú‚îÄ> Flag timing deviations with confidence
    ‚îÇ           ‚îî‚îÄ> Handle order-flexible steps

STEP 4: FALLBACK - DOCUMENT-DRIVEN IDENTIFICATION
    ‚îÇ
    ‚îú‚îÄ> If matter metadata incomplete:
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ> Extract from documents:
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> Court name from document headers
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> Jurisdiction from court name
    ‚îÇ   ‚îÇ   ‚îú‚îÄ> Act citations to identify applicable Acts
    ‚îÇ   ‚îÇ   ‚îî‚îÄ> Dates to determine Act version
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îî‚îÄ> Use extracted info to find templates
    ‚îÇ       ‚îî‚îÄ> Flag: "Jurisdiction inferred from documents - verify"

STEP 5: ATTORNEY VERIFICATION
    ‚îÇ
    ‚îî‚îÄ> Present selected template to attorney:
        ‚îú‚îÄ> Show: "Using Dematerialization template (Torts Act 1992, India)"
        ‚îú‚îÄ> Allow override: "Use different template?"
        ‚îî‚îÄ> Save attorney selection for future queries
```

### Example: Template Selection for Nirav Jobalia Case

```
MATTER METADATA:
‚îú‚îÄ> matter_id: "matter-123"
‚îú‚îÄ> jurisdiction: "India"
‚îú‚îÄ> court_name: "Bombay High Court"
‚îú‚îÄ> case_year: 2023
‚îî‚îÄ> applicable_acts: ["Torts Act 1992"]

QUERY: "Any lapses in dematerialization process?"

TEMPLATE SELECTION PROCESS:
1. Get matter context:
   ‚îî‚îÄ> Jurisdiction: "India", Year: 2023, Acts: ["Torts Act 1992"]

2. Identify process:
   ‚îî‚îÄ> Process: "Dematerialization"

3. Query templates:
   SQL: SELECT * FROM process_templates
        WHERE process_name = 'Dematerialization'
        AND jurisdiction = 'India'
        AND act_id IN (
            SELECT act_id FROM acts 
            WHERE act_name = 'Torts Act' 
            AND act_year = 1992
        )
        AND (applicable_years->>'start' <= 2023
             AND (applicable_years->>'end' IS NULL 
                  OR applicable_years->>'end' >= 2023))

4. Result:
   ‚îî‚îÄ> Template: "Dematerialization Process (Torts Act 1992, India)"
       ‚îú‚îÄ> Authority: Torts Act Section 12, 15
       ‚îú‚îÄ> 6 required steps
       ‚îî‚îÄ> Applicable: 1992 - present

5. Use template:
   ‚îî‚îÄ> Compare document events against 6 required steps
```

### Handling Multiple Jurisdictions

```
SCENARIO: Matter spans multiple jurisdictions

MATTER METADATA:
‚îú‚îÄ> jurisdiction: ["India", "UK"]
‚îú‚îÄ> applicable_acts: ["Torts Act 1992 (India)", "Companies Act 2006 (UK)"]

TEMPLATE SELECTION:
‚îú‚îÄ> Find templates for EACH jurisdiction
‚îú‚îÄ> Load both templates
‚îî‚îÄ> Compare process requirements:
    ‚îú‚îÄ> India: Dematerialization (Torts Act 1992) - 6 steps
    ‚îî‚îÄ> UK: Share Dematerialization (Companies Act 2006) - 8 steps

ANALYSIS:
‚îú‚îÄ> Compare documented actions against templates for BOTH jurisdictions
‚îú‚îÄ> Flag conflicts (e.g., different timeline requirements)
‚îî‚îÄ> Present both to attorney for determination
```

### Handling Act Amendments

```
SCENARIO: Act amended after case year

MATTER METADATA:
‚îú‚îÄ> case_year: 2020
‚îî‚îÄ> applicable_acts: ["Torts Act 1992"]

TEMPLATE SELECTION:
‚îú‚îÄ> Query templates:
‚îÇ   ‚îú‚îÄ> WHERE act_year <= 2020 (use version in force during case)
‚îÇ   ‚îî‚îÄ> NOT act_year > 2020 (don't use later amendments)
‚îÇ
‚îî‚îÄ> Result: Use Torts Act 1992 template (not 2023 amended version)

IMPORTANT: Templates are versioned by Act year, not template creation date
```

### Key Points

‚úÖ **Matter-level configuration** - Jurisdiction/court set during matter creation  
‚úÖ **Document extraction** - System extracts Act citations and process mentions from documents  
‚úÖ **Template matching** - Templates matched by jurisdiction + Act + year  
‚úÖ **Attorney verification** - Attorney confirms template selection  
‚úÖ **Version control** - Templates tied to specific Act versions/years  
‚úÖ **Multi-jurisdiction support** - Can handle matters spanning multiple jurisdictions  

‚ùå **No auto-detection** - System doesn't guess jurisdiction without matter metadata  
‚ùå **No web search** - All templates come from Act Knowledge Base  
‚ùå **No dynamic creation** - Templates must be pre-defined by legal experts

### Key Points

‚úÖ **Pre-defined templates** - Not web searched  
‚úÖ **Act-based** - All process requirements come from Acts  
‚úÖ **Structured database** - Stored in PostgreSQL  
‚úÖ **Manual creation** - Templates created by legal experts from Act text  
‚úÖ **Evidence-bound** - Every finding tied to Act section + document  

‚ùå **No web searching** - Process knowledge comes from Acts, not internet  
‚ùå **No dynamic discovery** - MVP uses pre-defined templates only  

---

## Stress Test & Safety Framework

LDIP has been designed to survive adversarial scrutiny from hostile senior advocates, conservative law firm partners, ethics committees, and real-world Indian litigation chaos.

### 10-Axis Stress Test

**Axis 1: Legal & Ethical Safety**
- **Challenge:** "LDIP is secretly giving legal advice"
- **Mitigation:** Language policing enforced at generation time, mandatory disclaimers on all outputs
- **Result:** ‚úÖ PASS - No outputs contain legal conclusion language
- **Boundary:** LDIP never asserts compliance, violation, ownership, or entitlement ‚Äî only the presence or absence of documentary evidence.

**Axis 2: Judicial Scrutiny**
- **Challenge:** "Where did this come from?" (Judge asking cold)
- **Mitigation:** Explainability mode shows exact text, location, reasoning for every finding
- **Result:** ‚úÖ PASS - Every signal is courtroom-defensible

**Axis 3: Indian Pleading Reality**
- **Challenge:** "Indian pleadings are sloppy ‚Äî your system will break"
- **Mitigation:** Indian Drafting Tolerance Layer, boilerplate recognition, graceful degradation
- **Result:** ‚úÖ PASS - LDIP degrades gracefully, not aggressively

**Axis 4: Bad Junior Lawyer Misuse**
- **Challenge:** "Junior blindly pastes LDIP output into court"
- **Mitigation:** Watermarks, export restrictions, explicit acknowledgements required
- **Result:** ‚úÖ PASS - Friction intentionally added to prevent misuse

**Axis 5: Overconfident Senior Advocate**
- **Challenge:** "This is obvious nonsense"
- **Mitigation:** Allow dismiss/override with reason, no automatic learning from overrides
- **Result:** ‚úÖ PASS - LDIP does not argue back

**Axis 6: Factual Ambiguity & Missing Records**
- **Challenge:** "You are guessing because documents are missing"
- **Mitigation:** Three-state logic only (Present / Explicitly absent / Not determinable)
- **Result:** ‚úÖ PASS - Uncertainty is first-class

**Axis 7: Cross-Matter Contamination**
- **Challenge:** "Is this using knowledge from other cases?"
- **Mitigation:** Strict matter isolation, explicit comparison labels
- **Result:** ‚úÖ PASS - No cross-matter data without explicit authorization

**Axis 8: Document Fabrication & Fraud Claims**
- **Challenge:** "You are accusing my client of forgery"
- **Mitigation:** Neutral language ("inconsistent formatting" not "forged"), "No conclusion drawn"
- **Result:** ‚úÖ PASS - LDIP never assigns intent

**Axis 9: Regulatory / Bar Council Review**
- **Challenge:** "This is unauthorized practice of law"
- **Mitigation:** No legal advice, no strategy, no outcomes, evidence-only, attorney-in-loop
- **Result:** ‚úÖ PASS - Defensible as forensic reading assistant

**Axis 10: Product Trust & Adoption**
- **Challenge:** "This slows me down"
- **Mitigation:** Signal ranking (Critical/Review/Informational), collapsible views
- **Result:** ‚úÖ PASS - Signal-to-noise controlled

### Safety Features

- **Query Guardrails:** Prevents unsafe queries, rewrites dangerous questions
- **Language Policing:** Real-time enforcement of neutral language
- **Attorney Verification:** Every finding requires attorney review
- **Explainability Mode:** Complete transparency for all findings
- **Cultural Sensitivity:** Understands Indian legal practice realities
- **Confidence Calibration:** Clear indication of certainty levels

LDIP succeeds not by replacing junior lawyers, but by enforcing the discipline that good juniors already follow and bad juniors skip.

## Summary

### The Problem
Legal teams struggle to analyze hundreds of documents manually, missing critical patterns, inconsistencies, and deviations that can impact case outcomes.

### The Solution
LDIP is an AI-assisted forensic reading assistant that automatically reads documents, extracts facts, detects inconsistencies, verifies citations, and surfaces patterns requiring attorney investigation‚Äîall while maintaining strict matter isolation and evidence-first architecture.

### The Value
- **70% time savings** in document analysis
- **85%+ accuracy** vs. manual review
- **$5,100-$8,500 savings** per matter
- **Better case outcomes** through comprehensive analysis

### The Differentiators
1. Evidence-first architecture (every claim cited)
2. Matter isolation (strict ethical walls)
3. Eight specialized detection engines
4. Neutral fact extraction (no legal conclusions)
5. Attorney supervision built-in
6. Query guardrails and language policing
7. Stress test compliant (survives adversarial scrutiny)

**LDIP doesn't replace lawyers‚Äîit makes them more effective.**

---


